{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from langchain_groq import ChatGroq\n",
    "\n",
    "os.environ[\"GROQ_API_KEY\"] = \"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\"\n",
    "os.environ[\"TAVILY_API_KEY\"] = \"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\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from langchain_groq import ChatGroq\n",
    "\n",
    "TEMPERATURE=0.5\n",
    "LLM_MODEL_NAME=\"mixtral-8x7b-32768\"\n",
    "EMBED_MODEL_NAME=\"sentence-transformers/all-MiniLM-L6-v2\"\n",
    "MILVUS_URI=\"./milvus_example.db\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "AIMessage(content=\"Hello! How can I help you today? If you have any questions or need assistance with package tracking, shipping rates, or anything else related to USPS, please let me know. I'm here to help!\", additional_kwargs={}, response_metadata={'token_usage': {'completion_tokens': 45, 'prompt_tokens': 8, 'total_tokens': 53, 'completion_time': 0.067878101, 'prompt_time': 0.001768355, 'queue_time': 0.020904363000000002, 'total_time': 0.069646456}, 'model_name': 'mixtral-8x7b-32768', 'system_fingerprint': 'fp_c5f20b5bb1', 'finish_reason': 'stop', 'logprobs': None}, id='run-cc831ccd-0352-41f2-bb07-8e58e9bd900d-0', usage_metadata={'input_tokens': 8, 'output_tokens': 45, 'total_tokens': 53})"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "llm = ChatGroq(temperature=TEMPERATURE, model_name=LLM_MODEL_NAME, api_key=os.getenv(\"GROQ_API_KEY\"))\n",
    "llm.invoke(\"hi\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "llm.invoke(\"hi\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.chat_models import init_chat_model\n",
    "\n",
    "model = init_chat_model(\"llama3-8b-8192\", model_provider=\"groq\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "res = model.invoke(\"Hello, world!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "d = res.response_metadata[\"token_usage\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-02-26 09:18:43,953 | DEBUG | graph | Token usage: {'completion_tokens': 10, 'prompt_tokens': 14, 'total_tokens': 24, 'completion_time': 0.008333333, 'prompt_time': 0.002980471, 'queue_time': 0.015748617, 'total_time': 0.011313804}\n"
     ]
    }
   ],
   "source": [
    "from logger import graph_logger as logger\n",
    "\n",
    "logger.debug('Token usage: %s', d)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-02-26 09:19:34,022 | DEBUG | graph | {'completion_tokens': 10, 'prompt_tokens': 14, 'total_tokens': 24, 'completion_time': 0.008333333, 'prompt_time': 0.002980471, 'queue_time': 0.015748617, 'total_time': 0.011313804}\n"
     ]
    }
   ],
   "source": [
    "logger.debug('{0}'.format(d))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.schema import HumanMessage, AIMessage\n",
    "from langchain.prompts import ChatPromptTemplate, MessagesPlaceholder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "AIMessage(content=\"Hi i am an AI.\").content"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "import os\n",
    "import sqlite3\n",
    "import configparser\n",
    "\n",
    "from langchain_core.prompts import PromptTemplate\n",
    "from typing import Any, Literal\n",
    "from models import ChatState, Grade\n",
    "from abc import ABC, abstractmethod\n",
    "from langchain_groq import ChatGroq\n",
    "from vector_store import vector_store\n",
    "from langchain.schema import SystemMessage\n",
    "from logger import graph_logger as logger\n",
    "from langgraph.graph import StateGraph, START, END\n",
    "from langgraph.checkpoint.sqlite import SqliteSaver\n",
    "from langchain_community.tools import TavilySearchResults"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Grade(binary_score='yes')"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "from langchain.chat_models import init_chat_model\n",
    "\n",
    "llm = init_chat_model(\"llama3-8b-8192\", model_provider=\"groq\")\n",
    "structured_llm = llm.with_structured_output(Grade)\n",
    "prompt = PromptTemplate(\n",
    "    template=\"\"\"You are a grader assessing relevance of a retrieved document to a user query. \\n \n",
    "    Here is the retrieved document: \\n\\n {context} \\n\\n\n",
    "    Here is the user query: {query} \\n\n",
    "    If the document contains keyword(s) or semantic meaning related to the user query, grade it as relevant. \\n\n",
    "    Give a binary score 'yes' or 'no' score to indicate whether the document is relevant to the query.\"\"\",\n",
    "    input_variables=[\"context\", \"query\"],\n",
    ")\n",
    "chain = prompt | structured_llm\n",
    "\n",
    "scored_result = chain.invoke({\"query\": \"slowness\", \"context\": \"slow as a turtle\"})\n",
    "#score = scored_result.binary_score\n",
    "scored_result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'yes'"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "scored_result.binary_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "chatapp",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
