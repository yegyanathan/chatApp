{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import sqlite3\n",
    "import argparse\n",
    "\n",
    "from pydantic import BaseModel\n",
    "from typing import Optional, List\n",
    "from langchain_groq import ChatGroq\n",
    "from langchain_milvus import Milvus\n",
    "from langgraph.checkpoint.sqlite import SqliteSaver\n",
    "from langchain.schema import HumanMessage, AIMessage\n",
    "from langchain_community.tools import TavilySearchResults\n",
    "from langchain_huggingface import HuggingFaceEmbeddings\n",
    "from langgraph.graph import StateGraph, START, END, add_messages\n",
    "from langchain.prompts import ChatPromptTemplate, MessagesPlaceholder\n",
    "\n",
    "os.environ[\"GROQ_API_KEY\"] = \"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\"\n",
    "os.environ[\"TAVILY_API_KEY\"] = \"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\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "########## Defaults ##########\n",
    "\n",
    "TEMPERATURE=0.5\n",
    "LLM_MODEL_NAME=\"mixtral-8x7b-32768\"\n",
    "EMBED_MODEL_NAME=\"sentence-transformers/all-MiniLM-L6-v2\"\n",
    "MILVUS_URI=\"./milvus_example.db\"\n",
    "\n",
    "########## Components ##########\n",
    "\n",
    "print(\"Setting up LLM...\")\n",
    "llm = ChatGroq(temperature=TEMPERATURE, model_name=LLM_MODEL_NAME)\n",
    "\n",
    "print(\"Setting up embed model...\")\n",
    "embeddings = HuggingFaceEmbeddings(model_name=EMBED_MODEL_NAME)\n",
    "\n",
    "print(\"Setting up Milvus vector DB...\")\n",
    "vector_db = Milvus(\n",
    "    embedding_function=embeddings,\n",
    "    connection_args={\"uri\": MILVUS_URI},\n",
    "    index_params={\"index_type\": \"FLAT\", \"metric_type\": \"L2\"},\n",
    "    auto_id=True\n",
    ")\n",
    "\n",
    "print(\"Setting up Tavily tool...\")\n",
    "web_search = TavilySearchResults(\n",
    "    max_results=1,\n",
    "    search_depth=\"advanced\",\n",
    "    include_answer=True,\n",
    "    include_raw_content=True,\n",
    "    include_images=True\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "########## Schema ##########\n",
    "\n",
    "class ChatState(BaseModel):\n",
    "    do_rag: bool = False\n",
    "    do_web_search: bool = False\n",
    "    rag_context: Optional[str] = \"\"\n",
    "    web_context: Optional[str] = \"\"\n",
    "    curr_query: Optional[str] = \"\"\n",
    "    curr_response: Optional[str] = \"\"\n",
    "    messages: List[HumanMessage | AIMessage] = []\n",
    "\n",
    "########## Nodes ##########\n",
    "    \n",
    "# Routing Function\n",
    "def router(state: ChatState):\n",
    "    print(\"state in router-\", state)\n",
    "    if state.do_rag and state.do_web_search:\n",
    "        return [\"rag_node\", \"web_search_node\"]\n",
    "    elif state.do_rag:\n",
    "        return [\"rag_node\"]\n",
    "    elif state.do_web_search:\n",
    "        return [\"web_search_node\"]\n",
    "    else:\n",
    "        return [\"llm_node\"]\n",
    "\n",
    "# RAG Node\n",
    "def rag_node(state: ChatState) -> ChatState:\n",
    "    print(\"state in rag-\", state)\n",
    "    \"\"\"Retrieve context from vector database.\"\"\"\n",
    "    context = vector_db.similarity_search(state.curr_query, k=3)\n",
    "    context = \"\\n\".join([doc.page_content for doc in context])\n",
    "    return {\"rag_context\": context}\n",
    "\n",
    "# Web Search Node\n",
    "def web_search_node(state: ChatState) -> ChatState:\n",
    "    print(\"state in web-\", state)\n",
    "    \"\"\"Retrieve context from web search.\"\"\"\n",
    "    context = web_search.run(state.curr_query)[0]['content']\n",
    "    return {\"web_context\": context}\n",
    "\n",
    "prompt_template = ChatPromptTemplate.from_messages([\n",
    "    (\"system\", \"You are a helpful assistant\"),\n",
    "    MessagesPlaceholder(\"messages\"),\n",
    "    (\"human\", \"{prompt}\")\n",
    "])\n",
    "\n",
    "# LLM Node to handle conversation\n",
    "def llm_node(state: ChatState) -> ChatState:\n",
    "    \"\"\"Generate response using LLM while maintaining conversation history.\"\"\"\n",
    "\n",
    "    print(\"state in llm-\", state)\n",
    "\n",
    "    context_parts = []  # Collect available contexts dynamically\n",
    "    \n",
    "    if state.rag_context:\n",
    "        context_parts.append(f\"RAG Context:\\n{state.rag_context}\")\n",
    "    if state.web_context:\n",
    "        context_parts.append(f\"Web Context:\\n{state.web_context}\")\n",
    "    \n",
    "    # Join all available parts with spacing\n",
    "    context_str = \"\\n\\n\".join(context_parts) if context_parts else \"No additional context available.\"\n",
    "    \n",
    "    # Final formatted string\n",
    "    prompt = f\"User Query:\\n{state.curr_query}\\n\\n{context_str}\"\n",
    "    \n",
    "    # Extract past conversation messages and format as history\n",
    "    prompt = prompt_template.format(messages=state.messages, prompt=prompt)\n",
    "    \n",
    "    print(\"llm invoke....\")\n",
    "    # Generate response using LLM\n",
    "    #response = llm.invoke(prompt).content\n",
    "    \n",
    "    # Update conversation history\n",
    "    state.messages.append(AIMessage(content=\"Hello\"))\n",
    "\n",
    "    return {\"message\": state.messages, \"curr_response\": \"hello\"}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Compiling graph...\n"
     ]
    }
   ],
   "source": [
    "########## Graph ##########\n",
    "\n",
    "conn = sqlite3.connect(\"checkpoints.sqlite\", check_same_thread=False)\n",
    "memory = SqliteSaver(conn)\n",
    "\n",
    "# Build the LangGraph workflow\n",
    "graph = StateGraph(ChatState)\n",
    "graph.add_node(\"rag_node\", rag_node)\n",
    "graph.add_node(\"web_search_node\", web_search_node)\n",
    "graph.add_node(\"llm_node\", llm_node)\n",
    "\n",
    "# Define edges\n",
    "graph.add_conditional_edges(START, router)\n",
    "graph.add_edge(\"rag_node\", \"llm_node\")\n",
    "graph.add_edge(\"web_search_node\", \"llm_node\")\n",
    "graph.add_edge(\"llm_node\", END)  # Ensure LLM output leads to END\n",
    "print(\"Compiling graph...\")\n",
    "graph = graph.compile(checkpointer=memory)\n",
    "# graph = graph.compile()\n",
    "\n",
    "def invoke_conversation(query, do_web_search, do_rag, thread_id):\n",
    "    \"\"\"Handles new and ongoing conversations based on thread_id.\"\"\"\n",
    "    state = ChatState(curr_query=query, do_web_search=do_web_search, do_rag=do_rag)\n",
    "    config = {\"configurable\": {\"thread_id\": thread_id}}\n",
    "    result = graph.invoke(state, config=config)\n",
    "    return result[\"response\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from IPython.display import Image, display\n",
    "from langchain_core.runnables.graph import MermaidDrawMethod\n",
    "\n",
    "display(\n",
    "    Image(\n",
    "        graph.get_graph().draw_mermaid_png(\n",
    "            draw_method=MermaidDrawMethod.API,\n",
    "        )\n",
    "    )\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting...\n",
      "Invoking graph...\n",
      "state in router- do_rag=False do_web_search=False rag_context='' web_context='' curr_query='My name is yegyanathan' curr_response='hello' messages=[]\n",
      "state in llm- do_rag=False do_web_search=False rag_context='' web_context='' curr_query='My name is yegyanathan' curr_response='hello' messages=[]\n",
      "llm invoke....\n",
      "{'do_rag': False, 'do_web_search': False, 'curr_query': 'My name is yegyanathan', 'curr_response': 'hello'}\n"
     ]
    }
   ],
   "source": [
    "print(\"Starting...\")\n",
    "\n",
    "state = ChatState(\n",
    "    do_rag=False,\n",
    "    do_web_search=False,\n",
    "    curr_query=\"My name is yegyanathan\"\n",
    ")\n",
    "\n",
    "config = {\"configurable\": {\"thread_id\": \"1\"}}\n",
    "print(\"Invoking graph...\")\n",
    "result = graph.invoke(state, config=config)\n",
    "print(result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "ChatState(do_rag=False, do_web_search=False, rag_context='', web_context='', curr_query='Hi how are you?', curr_response='', messages=[])"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "state"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "conn = sqlite3.connect(\"checkpoints.sqlite\", check_same_thread=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('checkpoints',), ('writes',)]\n"
     ]
    }
   ],
   "source": [
    "cursor = conn.cursor()\n",
    "cursor.execute(\"SELECT name FROM sqlite_master WHERE type='table';\")\n",
    "tables = cursor.fetchall()\n",
    "print(tables)  # List of table names"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(0, 'thread_id', 'TEXT', 1, None, 1)\n",
      "(1, 'checkpoint_ns', 'TEXT', 1, \"''\", 2)\n",
      "(2, 'checkpoint_id', 'TEXT', 1, None, 3)\n",
      "(3, 'task_id', 'TEXT', 1, None, 4)\n",
      "(4, 'idx', 'INTEGER', 1, None, 5)\n",
      "(5, 'channel', 'TEXT', 1, None, 0)\n",
      "(6, 'type', 'TEXT', 0, None, 0)\n",
      "(7, 'value', 'BLOB', 0, None, 0)\n"
     ]
    }
   ],
   "source": [
    "# Get table schema\n",
    "table_name = \"writes\"\n",
    "cursor.execute(f\"PRAGMA table_info({table_name})\")\n",
    "\n",
    "# Fetch and print results\n",
    "columns = cursor.fetchall()\n",
    "for col in columns:\n",
    "    print(col)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<sqlite3.Cursor at 0x13b517dc0>"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cursor.execute(f\"SELECT * from writes\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "('1', '', '1eff0ccf-fcc6-6bbe-bfff-a17a6d99b3d8', 'cd1f5fdc-7bda-7beb-9cd5-c839dd731933', 0, 'use_rag', 'msgpack', b'\\xc2')\n",
      "('1', '', '1eff0ccf-fcc6-6bbe-bfff-a17a6d99b3d8', 'cd1f5fdc-7bda-7beb-9cd5-c839dd731933', 1, 'use_web_search', 'msgpack', b'\\xc3')\n",
      "('1', '', '1eff0ccf-fcc6-6bbe-bfff-a17a6d99b3d8', 'cd1f5fdc-7bda-7beb-9cd5-c839dd731933', 2, 'messages', 'msgpack', b'\\x91\\xc7\\xdf\\x05\\x94\\xbdlangchain_core.messages.human\\xacHumanMessage\\x87\\xa7content\\xd9+Who is the current chief minister of delhi?\\xb1additional_kwargs\\x80\\xb1response_metadata\\x80\\xa4type\\xa5human\\xa4name\\xc0\\xa2id\\xd9$9028a7bf-43ad-4632-a84d-0259db227f96\\xa7example\\xc2\\xb3model_validate_json')\n",
      "('1', '', '1eff0ccf-fcc6-6bbe-bfff-a17a6d99b3d8', 'cd1f5fdc-7bda-7beb-9cd5-c839dd731933', 3, 'rag_context', 'msgpack', b'\\xa0')\n",
      "('1', '', '1eff0ccf-fcc6-6bbe-bfff-a17a6d99b3d8', 'cd1f5fdc-7bda-7beb-9cd5-c839dd731933', 4, 'web_context', 'msgpack', b'\\xa0')\n",
      "('1', '', '1eff0ccf-fcc6-6bbe-bfff-a17a6d99b3d8', 'cd1f5fdc-7bda-7beb-9cd5-c839dd731933', 5, 'branch:__start__:router:web_search_node', 'msgpack', b'\\xa9__start__')\n",
      "('1', '', '1eff0ccf-fcdb-67a8-8000-464128f00fe0', '24b54660-1020-108f-5f54-99fd6a2bc00c', 0, 'web_context', 'msgpack', b\"\\xda\\x02\\xd1Delhi CM List: Chief Ministers of Delhi Party Names and Tenure till 2024 Following that, elections are held every five years to elect Delhi's Chief Minister. The current Chief Minister of New Delhi was Shri Arvind Kejriwal, who resigned from his position on September 15, 2024. List of Chief Minister of Delhi with Party Name and Work Duration Here is a list of Delhi's Chief Ministers, along with their party names and terms in office. He became the Chief Minister of Delhi on December 2, 1993, serving until February 26, 1996. Reiterating the AAP's call for early elections in Delhi, party leader Gopal Rai said Tuesday that Atishi will serve as chief minister until Arvind Kejriwal is re-elected with a large majority.\")\n",
      "('1', '', '1eff0ccf-fcdb-67a8-8000-464128f00fe0', '24b54660-1020-108f-5f54-99fd6a2bc00c', 1, 'web_search_node', 'msgpack', b'\\xafweb_search_node')\n",
      "('1', '', '1eff0cd0-5c25-6e98-8001-887f734fa0bc', '77e5dc9f-b1a4-1587-385c-55e9604cf020', 0, 'messages', 'msgpack', b'\\x91\\xc8\\x03\\xb0\\x05\\x94\\xbalangchain_core.messages.ai\\xa9AIMessage\\x8a\\xa7content\\xda\\x01\\xaeBased on the information provided, Arvind Kejriwal was the Chief Minister of Delhi before resigning on September 15, 2024. However, according to the most recent update, Atishi will serve as the interim Chief Minister until Arvind Kejriwal is re-elected with a majority. Therefore, as of now, Atishi is the acting Chief Minister of Delhi. Please note that this information is based on the provided context and may change over time.\\xb1additional_kwargs\\x80\\xb1response_metadata\\x85\\xabtoken_usage\\x87\\xb1completion_tokensh\\xadprompt_tokens\\xcc\\xd0\\xactotal_tokens\\xcd\\x018\\xafcompletion_time\\xcb?\\xc4\\x89\\xb1(E]F\\xabprompt_time\\xcb?\\x8b\\x06\\xac}\\xb3\\x1aZ\\xaaqueue_time\\xcb?\\x94>\\xd9{\\xff!U\\xaatotal_time\\xcb?\\xc6:\\x1b\\xf0 \\x8e\\xec\\xaamodel_name\\xb2mixtral-8x7b-32768\\xb2system_fingerprint\\xadfp_c5f20b5bb1\\xadfinish_reason\\xa4stop\\xa8logprobs\\xc0\\xa4type\\xa2ai\\xa4name\\xc0\\xa2id\\xd9*run-ce79fdd8-88cf-4711-8a4d-e58173620a23-0\\xa7example\\xc2\\xaatool_calls\\x90\\xb2invalid_tool_calls\\x90\\xaeusage_metadata\\x83\\xacinput_tokens\\xcc\\xd0\\xadoutput_tokensh\\xactotal_tokens\\xcd\\x018\\xb3model_validate_json')\n",
      "('1', '', '1eff0cd0-5c25-6e98-8001-887f734fa0bc', '77e5dc9f-b1a4-1587-385c-55e9604cf020', 1, 'llm_node', 'msgpack', b'\\xa8llm_node')\n"
     ]
    }
   ],
   "source": [
    "columns = cursor.fetchall()\n",
    "for col in columns:\n",
    "    print(col)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "chatapp",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
